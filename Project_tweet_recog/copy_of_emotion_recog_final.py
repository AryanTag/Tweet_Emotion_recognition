# -*- coding: utf-8 -*-
"""Copy_of_emotion_recog_final.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ObkpHNLv4lUqwtsxAauRPnHVPo0qckpY
"""

pip install nlp

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline

import tensorflow as tf
import pandas as pd
import matplotlib.pyplot as plt
import nlp
import numpy as np

twitter_dataset = nlp.load_dataset('emotion')

twitter_dataset #The dataset is already split into train, test and validation sets

train_data=twitter_dataset["train"]
test_data=twitter_dataset["test"]
val_data=twitter_dataset["validation"]

train_text=train_data['text']
train_labels=train_data['label']
test_text=test_data['text']
test_labels=test_data['label']
val_text=val_data['text']
val_labels=val_data['label']

(train_text[2], train_labels[2])

from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

token=Tokenizer(num_words=12000, oov_token="<UNK>")
token.fit_on_texts(train_text)

def text_to_seq(text):
  seq=token.texts_to_sequences(text)
  return seq



### CALLING ALL THE FUNCTIONS TO GET TRAIN, TEST, VAL SEQUENCES

train_seq=text_to_seq(train_text)
test_seq=text_to_seq(test_text)
val_seq=text_to_seq(val_text)

#CHECKING IF THE SEQUENCES ARE CORRECT
print(train_seq[1])   
print(train_text[1])
print(test_seq[1])   
print(test_text[1])
print(val_seq[1])   
print(val_text[1])

len_t=[(np.array(tex.split()).shape[0]) for tex in train_text]

# CODE BLOCK FOR PADDING OF THE SEQUENCES
# FIRST DECIDING HOW MANY WORDS TO TAKE FORM THE ORIGIAL SENTENCES

plt.hist(len_t, bins=np.array(len_t).max()-1)
plt.show()

"""### AFTER OBSERVING THAT MOST OF THE TWEETS ARE HAVING LENGTH LESS THAT 55 WORDS ###"""

max_len=55
train_padded=pad_sequences(train_seq, maxlen=max_len, padding='post', truncating='pre')
test_padded=pad_sequences(test_seq, maxlen=max_len, padding='post', truncating='pre')
val_padded=pad_sequences(val_seq, maxlen=max_len, padding='post', truncating='pre')

train_padded[1]

# MAKING THE LABELS
label=dict((c,i) for (i,c) in enumerate(set(train_labels)))
label2=dict((i,c) for (i,c) in enumerate(set(train_labels)))
train_label=np.array([label.get(num) for num in train_labels])
test_label=np.array([label.get(num) for num in (test_labels)])
val_label=np.array([label.get(num) for num in (val_labels)])

label

(train_text[2], train_labels[2], train_label[2])

# DEFINING THE ACTUAL MDOEL

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, LSTM, Embedding, Bidirectional

model=Sequential([
                  Embedding(12000,64, input_length=max_len),
                  Bidirectional(LSTM(64, return_sequences=True)),
                  Bidirectional(LSTM(64, return_sequences= True)),
                  Bidirectional(LSTM(64, dropout=0.05 )),
                  Dense(6, activation='softmax')
])
model.compile(metrics='accuracy', loss='sparse_categorical_crossentropy', )

model.summary()

#TRAINING THE MODEL

his=model.fit(train_padded, train_label, validation_data=(val_padded, val_label), epochs=10)

from google.colab import drive
drive.mount('/content/gdrive')

#model.save_weights("/content/gdrive/MyDrive/kaggle/emotion_1/my_model")

#model.load_weights("/content/gdrive/MyDrive/kaggle/emotion_1/my_model")

model.evaluate(train_padded, train_label)

plt.figure(figsize=(16, 6))

plt.plot(his.history.get('accuracy'), label='Training')
plt.plot(his.history.get('val_accuracy'), label='Validation')
plt.ylim([0., 1.])
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()
plt.show()

plt.figure(figsize=(16, 6))

plt.plot(his.history.get('loss'), label='Training')
plt.plot(his.history.get('val_loss'), label='Validation')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()

model.evaluate(test_padded, test_label)

## FUNCTION FOR CUSTOM TEXT PREDICTION
def predict(text):
  token.fit_on_texts(text)
  text_seq=text_to_seq([text])
  text_padded=pad_sequences(text_seq, maxlen=max_len, padding='post', truncating='pre')
  p=model.predict(text_padded)
  pred=label2.get(np.argmax(p))
  return pred

# TESTING ON OUR OWN TEXT
text="dogs are cute"
print("THE PREDICTED EMOTION IS : {}".format(predict(text)))

"""### STREAMING THE TWEETS USING TWITTER API ###"""

!pip install emoji

# MAKING THE NECESSARY IMPORTS
import os
import tweepy as tw
import pandas as pd
import pandas as pd
import re
import emoji
import nltk
nltk.download('words')
words = set(nltk.corpus.words.words())

# THESE KEYS WILL BE AVAILABLE IN THE TWITTER DEVELOPER ACCOUNT

consumer_key="QSYgnBReKVesOtrIhbdYPNWSs"
consumer_secret="RHL8lhTBhemstAMoQR1cSuSNFPBZWGuW4A6K5nB9TDLptI3QkJ"
access_token="1418540234929676291-1c0HrVBQDvnN0ycMiuQvhD7gtcwbgs"
access_token_secret="dGW1e4VJwalraRvux73FYGDkqqhln5DHtHvYj8lQJCKwh"

auth = tw.OAuthHandler(consumer_key, consumer_secret)
auth.set_access_token(access_token, access_token_secret)
api = tw.API(auth, wait_on_rate_limit=True)

"""### SEARCHING FOR TWEETS CONTAINING "THE" & CLEANING THE TWEET DATASET
Removing special symbols like @, removing https links, removing hashtags etc
"""

# FOR CLEANING AND GETTING TWEET DATA
new_search = "NEWS"
date_since = "2010-11-16"
tweets = tw.Cursor(api.search,
              q=new_search,
              lang="en",
              since=date_since).items(10)
def cleaner(tweet):
    tweet = re.sub("@[A-Za-z0-9]+","",tweet) 
    tweet = re.sub(r"(:?:\@|http?\://|https?\://|www)\S+", "", tweet) 
    tweet = " ".join(tweet.split())
    tweet = ''.join(c for c in tweet if c not in emoji.UNICODE_EMOJI) 
    tweet = tweet.replace("#", "").replace("_", " ") 
    tweet = " ".join(w for w in nltk.wordpunct_tokenize(tweet) \
         if w.lower() in words or not w.isalpha())
    return tweet

# STORING THE ORIGINAL TWEETS IN array_org
array_org=[]
for tweet in tweets:
    array_org.append((tweet.text))

array_org

# STORING THE CLEANED TWEETS IN array_org
array=[]
for arr in array_org:
    array.append(cleaner(arr))
array

#i=np.random.randint(10)
i=
print("ORIGINAL TWEET : {}\n".format(array_org[i]))
print("PREDICTED EMOTION OF THE TWEET : '{}' is = {}".format(array[i], predict(array[i])))

